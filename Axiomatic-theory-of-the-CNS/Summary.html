<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="generator" content="pandoc">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
    <meta name="author" content="Aegithales">

    <meta name="theme-color" media="(prefers-color-scheme: light)" content="white">
    <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1f2937">

    
    <title> “Summary of the key insights”</title>

    <link rel="icon" type="image/x-icon" href="../img/favicon.ico">
    <link rel="stylesheet" href="../blog.css">

    <!-- this strange thing is needed for syntax highlighting... -->
    <style>
        code{white-space: pre-wrap;}
        span.smallcaps{font-variant: small-caps;}
        span.underline{text-decoration: underline;}
        div.column{display: inline-block; vertical-align: top; width: 50%;}
        div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
        ul.task-list{list-style: none;}
    </style>
</head>

<body>
    <div class="container">
        <nav>
            <img src="../img/mascot-small.png" alt="mascot-small.png">
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="About.html">About</a></li>
                <li><a href="Contact.html">Contact</a></li>
            </ul>
        </nav>

        <main>
            <h2
            id="the-current-status-of-my-axiomatic-theory-of-the-mind-behaviour">The
            current status of my ‘axiomatic’ theory of the mind /
            behaviour</h2>
            <ul>
            <li><p>  My goal is to build an ‘axiomatic’ theory of neural
            architectures,     a theory that is agnostic of the
            underlying neuron model (McCulloch–Pitts, spiking, HTM, etc)
                and the learning algorithm (Hebbian, Oja, free-energy,
            BP, STDP, Bayesian…)</p></li>
            <li><p>Stabilizer: a ‘predictor’ module, defined by its
            <em>convergence</em> behaviour, not by its output.</p>
            <p>Its parameters (eg weights) ‘converge’ when the input it
            <em>predictable</em>, and ‘diverge’ when the input is
            unpredictable / random / noise-like.</p></li>
            <li><p>Its main use is to loop back its output as an
            additional input, paired with the external input, so that
            the bundle will be predictable.</p>
            <p>In this system, the Stabilizer maintains a <em>state</em>
            that makes the input predictable, implementing ‘perception’:
            integrating successive sensory inputs to build a consistent
            model of the surroundings (‘scene’).</p></li>
            <li><p>The ‘transformer loop’ architecture, connecting two
            <em>stabilizers</em>, one as the ‘encoder’, the other as the
            ‘decoder’.</p></li>
            <li><p>Reinforcement Learning: defined similarly by its
            <em>convergence</em> behaviour: it converges when the global
            RL signal is positive, and diverges when it is
            negative.</p></li>
            <li><p>Implementing the Stabilizer by an STDP-like update
            rule, for RANs, Retained Activation Neurons.</p></li>
            <li><p>The Stabilizer can discover the state-transition
            digraph of an FSM, by observing its output (maybe also by
            triggering the next state transition).</p></li>
            <li><p>2D spatial maps are FSMs, so the Stabilizer can learn
            2D navigation, implementing ‘path integration’. No need for
            pre-wired ‘grid cells’.</p></li>
            <li><p>The FSM digraph can be re-used to explore other,
            similar FSMs. Eg the grid layout FSM is the same for each 2D
            locations, so it can be used as a <em>prior</em> for
            learning the new map.</p></li>
            <li><p>Using FSM priors speeds up learning and allows
            <em>learning by analogy</em>. Eg counting is an FSM digraph,
            reused for counting different objects.</p></li>
            <li><p>The Stabilizer with RL can be the top-box module for
            animals, seeking to maximize some rewards. Hard-wired
            actions may produce reward signals when successful.</p>
            <p>This is enough to model some simple behaviours, like
            Foraging.</p></li>
            </ul>
            <h2 id="unsolved-problems">Unsolved problems</h2>
            <ul>
            <li><p>Intent: how intent is formed in the top-box
            module?</p></li>
            <li><p>Delayed reward: how RL works / controls convergence
            when the reward is delayed (received only after N steps,
            where N is high…)</p></li>
            <li><p>How multistep Intent (aka a Plan) is formed /
            stored?</p></li>
            <li><p>How a Plan is <em>rehearsed</em> in the mind? How
            actions are ‘simulated’ before taken?</p></li>
            <li><p>How <em>instances</em> of known classes, like
            persons, objects, locations, are remembered? Where these
            memories are stored?</p></li>
            <li><p>How memories are retained during deep sleep and
            anaesthesia? If neuron activation is not sustained in sleep
            / unconscious state why there is no loss of memories after
            surgery in general anaesthesia?</p></li>
            <li><p>Can synapses be used to store large amount of data
            persistently? That is, storing data in synapses, as opposed
            to remembering by sustaining an activation pattern?</p></li>
            </ul>
        </main>
    </div>

</body>
</html>
